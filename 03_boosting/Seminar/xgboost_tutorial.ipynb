{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <font color='blue'>XGBoost</font> eXtreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://github.com/dmlc/xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем датасет Boston Housing и обучим XGBoost на нем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "\n",
    "\n",
    "rng = np.random.RandomState(31337)\n",
    "\n",
    "boston = load_boston()\n",
    "y = boston['target']\n",
    "X = boston['data']\n",
    "\n",
    "kf = KFold(n_splits=2, shuffle=True, random_state=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost предлагает 2 способа использования алгоритмов:\n",
    "* sklearn-совместимые классы XGBClassifier, XGBRegressor\n",
    "\n",
    "* \"оригинальная\" python-библиотека"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_index, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    xgb_model = xgb.XGBRegressor().fit(X[train_index], y[train_index])\n",
    "    predictions = xgb_model.predict(X[test_index])\n",
    "    actuals = y[test_index]\n",
    "    print(\"RMSE on fold {}: {}\".format(fold_index, np.sqrt(mean_squared_error(actuals, predictions))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params():\n",
    "    params = {}\n",
    "    params[\"objective\"] = \"reg:linear\"\n",
    "    params[\"booster\"] = \"gbtree\"\n",
    "    params[\"eval_metric\"] = \"rmse\"\n",
    "    params[\"num_boost_round\"] = 100\n",
    "    params[\"max_depth\"] = 3\n",
    "    params[\"tree_method\"] = \"approx\"\n",
    "    params[\"sketch_eps\"] = 1\n",
    "    \n",
    "    return params\n",
    "    \n",
    "for fold_index, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "\n",
    "    params = get_params()\n",
    "    \n",
    "    xgtrain = xgb.DMatrix(X[train_index], label=y[train_index])\n",
    "    xgtest = xgb.DMatrix(X[test_index], label=y[test_index])\n",
    "\n",
    "    bst = xgb.train(params, xgtrain)\n",
    "\n",
    "    print(\"RMSE on fold {}: {}\".format(fold_index, bst.eval(xgtest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Градиентный бустинг на решающих деревьях"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "\n",
    "Хотим построить композицию алгоритмов:\n",
    "<font size=5>\n",
    "\n",
    "$$ \\hat{y_i} = \\phi(x_i) = \\sum_{k=1}^{K} f_k(x_i) $$\n",
    "\n",
    "$$ Obj(f) = \\sum_{i} l(y_i, \\hat{y_i} ) + \\sum_k \\Omega(f_k)$$\n",
    "\n",
    "$$ \\Omega(f_k) = \\gamma T + \\frac{1}{2}\\lambda\\sum_{j=1}^{T}w_j^2 + \\alpha\\sum_{j=1}^{T}w_j$$\n",
    "\n",
    "\n",
    "<font size=3>\n",
    "\n",
    "$ x_i, y_i, \\hat{y_i} $ - i-ый объект, правильный ответ и предсказание модели для для него\n",
    "\n",
    "$ \\Omega $ - регуляризация\n",
    "\n",
    "T - количество листьев в дереве\n",
    "\n",
    "$ w_j $ - веса, проставленные в листьях дерева"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Преимущества:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* потенциально очень высокое качество во многих задачах\n",
    "\n",
    "* находит нелинейные связи\n",
    "\n",
    "* способен обработать датасеты с большим числом объектов и признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Недостатки:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* очень много параметров\n",
    "\n",
    "* модели не интерпретируемы\n",
    "\n",
    "* по умолчанию не очень быстрый"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Особенности XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "Написан на C++, есть обертки на Python, R, Java, Scala\n",
    "\n",
    "С помощью XGBoost выиграна половина конкурсов на Kaggle\n",
    "\n",
    "Существует коммерческая версия TreeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "Для уменьшения переобучения целевая функция поддерживает L0, L1, L2 регуляризации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Параллелизм (по признакам)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://zhanpengfang.github.io/fig_418/feature_speedup.png\" height=\"600\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "Также есть возможность запускаться на Hadoop, Spark, Flink и DataFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кастомные функции потерь / метрики качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В XGBoost встроено множество различных функций потерь:\n",
    "\n",
    "* reg:linear\n",
    "\n",
    "* reg:logistic\n",
    "\n",
    "* binary:logistic\n",
    "\n",
    "* binary:logitraw\n",
    "\n",
    "* multi:softmax\n",
    "\n",
    "* rank:pairwise\n",
    "\n",
    "* ...\n",
    "\n",
    "А также соответствующих eval_metric, которые замеряют качество и позволяют сделать early stop.\n",
    "\n",
    "Но также имеется возможность реализовать свой objective и eval_metric.\n",
    "\n",
    "Все, что для этого нужно - уметь считать градиент и гессиан."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_reg_linear(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    grad = (preds - labels)\n",
    "    hess = np.ones(labels.shape[0])\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    "    params = get_params()\n",
    "    \n",
    "    xgtrain = xgb.DMatrix(X[train_index], label=y[train_index])\n",
    "    xgtest = xgb.DMatrix(X[test_index], label=y[test_index])\n",
    "\n",
    "    bst = xgb.train(params, xgtrain, obj=my_reg_linear)\n",
    "    \n",
    "    predictions = bst.predict(xgtest)\n",
    "    actuals = y[test_index]\n",
    "\n",
    "    print(bst.eval(xgtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximated tree splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если данных слишком много, то можно использовать не все значения признаков, а разделить их на бакеты.\n",
    "\n",
    "А именно, от каждого признака берутся не все значения, а только некоторое подмножество. Разбиение производится по элементам этого подмножества. \n",
    "\n",
    "Для разбиения выбираются взвешенные перцентили.\n",
    "\n",
    "В оригинальной статье указывается 2 алгоритма:\n",
    "*   глобальный - один раз выбрать разбиение значений фактора перед началом построения дерева и зафиксировать\n",
    "\n",
    "    экономим на выборе разбиений, но обычно приходится выбирать больше точек разбиения\n",
    "    \n",
    "    \n",
    "*   локальный - выбирать разбиение после каждого сплита\n",
    "  \n",
    "    работает лучше на глубоких деревьях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"tree_method\"] = \"approx\"\n",
    "params[\"sketch_eps\"] = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пропуски в данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost умеет обрабатывать разреженные матрицы\n",
    "\n",
    "Но категориальные признаки нужно приводить к числовому виду\n",
    "\n",
    "Нужно указать, какое число является \"пропуском\"\n",
    "\n",
    "При сплите, алгоритм смотрит в какую сторону лучше отвести объекты с пропуском."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgtrain_missed = xgb.DMatrix(X[test_index], label=y[test_index], missing=-999.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчитывает сколько раз каждый признак использовался для использовался в вершине дерева при разбиении\n",
    "\n",
    "Это не качество фактора, а его важность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst.get_fscore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "xgb.plot_importance(bst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Прунинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно GBM перестает разделять вершины дерева, когда gain становится отрицательным - жадный подход.\n",
    "Могло оказаться так, что после неудачного сплита с отрицательным gain'ом получится сделать сильно положительный сплит.\n",
    "\n",
    "XGBoost доводит деревья до max_depth, после чего начинает удалять сплиты, которые несут отрицательный вклад."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дообучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix('agaricus.txt.train')\n",
    "dtest = xgb.DMatrix('agaricus.txt.test')\n",
    "watchlist  = [(dtest,'eval'), (dtrain,'train')]\n",
    "###\n",
    "# advanced: start from a initial base prediction\n",
    "#\n",
    "print ('start running example to start from a initial prediction')\n",
    "# specify parameters via map, definition are same as c++ version\n",
    "param = {'max_depth':2, 'eta':1, 'silent':1, 'objective':'binary:logistic' }\n",
    "# train xgboost for 1 round\n",
    "bst = xgb.train( param, dtrain, 1, watchlist )\n",
    "\n",
    "# Note: we need the margin value instead of transformed prediction in set_base_margin\n",
    "# do predict with output_margin=True, will always give you margin values before logistic transformation\n",
    "ptrain = bst.predict(dtrain, output_margin=True)\n",
    "ptest  = bst.predict(dtest, output_margin=True)\n",
    "\n",
    "dtrain.set_base_margin(ptrain)\n",
    "dtest.set_base_margin(ptest)\n",
    "\n",
    "print ('this is result of running from initial prediction')\n",
    "bst = xgb.train( param, dtrain, 1, watchlist )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Встроенная кросс валидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.cv(param, dtrain, nfold = 4, num_boost_round=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 вида бустеров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* gbtree - обычные решающие деревья\n",
    "\n",
    "* gblinear - линейные модели\n",
    "\n",
    "* dart - решающие деревья, алгоритм может \"выбрасывать\" некоторые из деревьев, уменьшая переобучение\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Веса для объектов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем учитывать каждый объект со своим весом, этот вес будет учитываться и при выборе бакетов при приближенном построении деревьев, при сплите, при подсчете Objective.\n",
    "\n",
    "Допустим, мы хотим классифицировать короткие сообщения.  Некоторые из них повторяются. В этом случае выгодно \"слить\" вместе все дубликаты и посчитать их один раз, но с большим весом. При неизменном качестве это уменьшит время обучения "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = np.random.randint(low=1, high=5, size=X.shape[0])\n",
    "train_examples = 300\n",
    "\n",
    "\n",
    "X_train = X[:train_examples]\n",
    "X_test = X[train_examples:]\n",
    "y_train = y[:train_examples]\n",
    "y_test = y[train_examples:]\n",
    "\n",
    "\n",
    "X_train_repeated = np.repeat(X_train, repeats[:train_examples], axis=0)\n",
    "X_test_repeated = np.repeat(X_test, repeats[train_examples:], axis=0)\n",
    "y_train_repeated = np.repeat(y_train, repeats[:train_examples], axis=0)\n",
    "\n",
    "\n",
    "xgtrain_repeated = xgb.DMatrix(X_train_repeated, label=y_train_repeated)\n",
    "xgtrain_weighted = xgb.DMatrix(X_train, label=y_train, weight=repeats[:train_examples])\n",
    "\n",
    "xgtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "bst = xgb.train(params, xgtrain_repeated)\n",
    "print(\"Repeated dataset. Train size: {}, error: {}\".format(xgtrain_repeated.num_row(), bst.eval(xgtest)))\n",
    "\n",
    "bst = xgb.train(params, xgtrain_weighted)\n",
    "print(\"Weighted dataset. Train size: {}, error: {}\".format(xgtrain_weighted.num_row(), bst.eval(xgtest)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Другие параметры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> learning_rates </i> - можно настроить убывающую скорость"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Параметры деревьев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "<i> max_depth </i> - максимальная глубина дерева. Слишком большая глубина ведет к переобучению\n",
    "\n",
    "<i> subsample, colsample_bytree, colsample_bylevel </i> - сэмплирование по объектам и признакам\n",
    "\n",
    "\n",
    "<i> min_child_weight </i> - минимальная сумма весов в листе\n",
    "\n",
    "<i> scale_pos_weight </i> - вес целого класса, используется если один класс заметно чаще встречается, чем другой\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополнительные параметры для DART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "<i> sample_type </i> - стратегия выбора деревьев для выкидывания\n",
    "\n",
    "<i> rate_drop </i> - какую долю выкидываем\n",
    "\n",
    "<i> skip_drop </i> - шанс пропустить дроп на этой итерации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Настраиваем XGBoost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "* Выбираем относительно большую learning_rate ($ \\eta \\in [0.05, 0.3]$), подбираем оптимальное число деревьев для выбранного $ \\eta $\n",
    "\n",
    "* Настраиваем параметры деревьев, начиная с самых значимых (max_depth, min_child_weight, gamma, subsample, colsample_bytree)\n",
    "\n",
    "* Настраиваем регуляризации ($ \\lambda, \\alpha $)\n",
    "\n",
    "* Уменьшаем learning_rate, пропорционально увеличиваем число деревьев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
